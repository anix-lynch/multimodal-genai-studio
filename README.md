# ğŸ¨ Multimodal GenAI Studio

A professional-grade multimodal AI application that generates and processes content across text, image, and audio modalities. Built to showcase expertise in multimodal generative AI and modern AI application development.

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Gradio](https://img.shields.io/badge/Gradio-4.0+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## ğŸŒŸ Features

### Text Generation
- **Multiple LLM Support**: Google Gemini, OpenAI GPT-4o, Anthropic Claude
- **Flexible Parameters**: Control temperature, max tokens, system prompts
- **Streaming Support**: Real-time text generation
- **FREE Option**: Gemini provides 60 requests/minute free

### Image Generation
- **DALL-E Integration**: DALL-E 2 and 3 with quality/style controls
- **Stable Diffusion**: Via HuggingFace API
- **Image Editing**: Edit existing images with prompts
- **Variations**: Generate variations of images
- **Multiple Formats**: Support for various sizes and aspect ratios

### Audio Processing
- **Transcription**: OpenAI Whisper API and local models
- **Multi-language**: Support for 50+ languages
- **Translation**: Automatic translation to English
- **Text-to-Speech**: OpenAI TTS (6 voices) and Google TTS
- **Voice Control**: Multiple voice options and speed control

### Multimodal Pipelines
- **Story to Multimedia**: Generate scene images + audio narration
- **Audio to Blog**: Transcribe audio â†’ formatted blog post + featured image
- **Creative Chains**: Iterative imageâ†’textâ†’image workflows
- **Text Roundtrip**: Test TTSâ†’transcription accuracy

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- At least ONE API key (Google, OpenAI, or Anthropic)

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/multimodal-genai-studio
cd multimodal-genai-studio

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys
cp .env.example .env
# Edit .env with your API keys
```

### Minimal Setup (FREE)

For a completely FREE setup, you only need:

```bash
# .env
GOOGLE_API_KEY=your_gemini_api_key
```

Get a FREE Gemini API key: https://makersuite.google.com/app/apikey

This enables:
- âœ… Text generation (Gemini)
- âœ… Text-to-speech (gTTS built-in)
- âœ… Basic multimodal pipelines

### Run Application

```bash
python app.py
```

Open browser to: http://localhost:7861

## ğŸ“š Documentation

### Configuration

The app supports multiple API providers. Configure in `.env`:

```bash
# === Required (at least one) ===
GOOGLE_API_KEY=your_key          # Gemini (FREE: 60 req/min)
OPENAI_API_KEY=your_key          # GPT, DALL-E, Whisper
ANTHROPIC_API_KEY=your_key       # Claude

# === Optional ===
HF_TOKEN=your_token              # Stable Diffusion
STABILITY_API_KEY=your_key       # Stability AI

# === Server Settings ===
HOST=0.0.0.0
PORT=7861
```

### Usage Examples

#### Text Generation
```python
from src.text.generator import TextGenerator

gen = TextGenerator()
result = gen.generate(
    prompt="Write a haiku about AI",
    model="gemini-1.5-flash",
    temperature=0.7
)
print(result['text'])
```

#### Image Generation
```python
from src.image.generator import ImageGenerator

gen = ImageGenerator()
result = gen.generate(
    prompt="A serene mountain landscape at sunset",
    model="dall-e-3",
    size="1024x1024"
)
print(f"Image saved to: {result['images'][0]}")
```

#### Audio Transcription
```python
from src.audio.transcriber import AudioTranscriber

trans = AudioTranscriber()
result = trans.transcribe(
    audio_path="audio.mp3",
    model="whisper-1"
)
print(result['text'])
```

#### Text-to-Speech
```python
from src.audio.synthesizer import TextToSpeech

tts = TextToSpeech()
result = tts.synthesize(
    text="Hello, this is a test",
    model="gtts",  # FREE option
    language="en"
)
print(f"Audio saved to: {result['audio_path']}")
```

#### Multimodal Pipeline
```python
from src.multimodal.pipeline import MultimodalPipeline

pipeline = MultimodalPipeline()
result = pipeline.story_to_multimedia(
    story="Once upon a time...",
    generate_images=True,
    generate_audio=True
)
print(f"Generated {len(result['images'])} images")
print(f"Audio: {result['audio_path']}")
```

## ğŸ—ï¸ Architecture

```
multimodal-genai-studio/
â”œâ”€â”€ app.py                          # Main Gradio application
â”œâ”€â”€ config.py                       # Configuration management
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ text/                       # Text generation
â”‚   â”‚   â””â”€â”€ generator.py            # Multi-provider LLM
â”‚   â”œâ”€â”€ image/                      # Image generation
â”‚   â”‚   â””â”€â”€ generator.py            # DALL-E + Stable Diffusion
â”‚   â”œâ”€â”€ audio/                      # Audio processing
â”‚   â”‚   â”œâ”€â”€ transcriber.py          # Speech-to-text
â”‚   â”‚   â””â”€â”€ synthesizer.py          # Text-to-speech
â”‚   â””â”€â”€ multimodal/                 # Multimodal workflows
â”‚       â””â”€â”€ pipeline.py             # Combined pipelines
â”‚
â”œâ”€â”€ outputs/                        # Generated content
â”‚   â”œâ”€â”€ images/                     # Generated images
â”‚   â”œâ”€â”€ audio/                      # Generated audio
â”‚   â”œâ”€â”€ transcriptions/             # Transcription outputs
â”‚   â””â”€â”€ multimodal/                 # Pipeline outputs
â”‚
â””â”€â”€ deployment/                     # Deployment configs
    â”œâ”€â”€ Dockerfile                  # Docker configuration
    â”œâ”€â”€ docker-compose.yml          # Docker Compose
    â””â”€â”€ README_HF_SPACES.md         # HuggingFace deployment
```

## ğŸ¯ Key Technologies

- **LLMs**: Google Gemini, OpenAI GPT-4o, Anthropic Claude
- **Image Generation**: DALL-E 2/3, Stable Diffusion XL
- **Audio**: OpenAI Whisper, OpenAI TTS, gTTS
- **Framework**: Gradio (Modern UI)
- **Backend**: Python 3.11+
- **Async**: Concurrent processing support

## ğŸš¢ Deployment

### Docker

```bash
docker-compose -f deployment/docker-compose.yml up
```

### HuggingFace Spaces

See [deployment/README_HF_SPACES.md](deployment/README_HF_SPACES.md) for detailed instructions.

Quick deploy:
1. Create Space on HuggingFace
2. Upload project files
3. Set API keys in secrets
4. Deploy (FREE on CPU Basic)

### Local Production

```bash
# Install production dependencies
pip install -r requirements.txt

# Run with gunicorn (recommended)
gunicorn app:app --bind 0.0.0.0:7861 --workers 4
```

## ğŸ’° Cost Analysis

### FREE Tier (Recommended for Personal Use)
- **Google Gemini**: 60 requests/min FREE
- **gTTS**: Unlimited FREE
- **HuggingFace**: Inference API FREE tier
- **Hosting**: HuggingFace Spaces CPU Basic FREE

**Total: $0/month** âœ…

### Paid Tier (Production)
- **OpenAI GPT-4o**: ~$10-50/month (typical usage)
- **DALL-E 3**: $0.04 per image
- **Whisper**: $0.006 per minute
- **TTS**: $15 per 1M characters
- **Hosting**: HuggingFace CPU Upgrade ~$22/month

**Total: $30-100/month** (varies by usage)

## ğŸ“ Skills Demonstrated

This project showcases expertise in:

### AI/ML
- âœ… Multimodal AI application development
- âœ… LLM integration and prompt engineering
- âœ… Image generation and manipulation
- âœ… Audio processing and synthesis
- âœ… Pipeline orchestration

### Software Engineering
- âœ… Clean architecture and modular design
- âœ… Error handling and logging
- âœ… Configuration management
- âœ… API integration best practices
- âœ… Production-ready code

### DevOps
- âœ… Docker containerization
- âœ… Environment management
- âœ… Deployment automation
- âœ… Multi-platform deployment

### Full Stack
- âœ… Modern UI with Gradio
- âœ… Backend API design
- âœ… File handling and storage
- âœ… Async processing

## ğŸ“Š Certifications

This project demonstrates skills from:
- **Build Multimodal Generative AI Applications** (IBM)
- Python for Data Science, AI & Development (IBM)
- Fundamentals of Building AI Agents (IBM)

## ğŸ”§ Development

### Run Tests
```bash
pytest tests/
```

### Code Quality
```bash
# Format code
black src/ app.py config.py

# Type checking
mypy src/

# Linting
pylint src/
```

### Add New Models

To add a new LLM provider:
1. Update `ModelConfig.TEXT_MODELS` in `config.py`
2. Add provider initialization in `TextGenerator.__init__`
3. Implement `_generate_{provider}` method

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork repository
2. Create feature branch
3. Add tests for new features
4. Submit pull request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- Google Gemini for FREE LLM access
- OpenAI for powerful multimodal APIs
- Anthropic for Claude
- HuggingFace for Stable Diffusion
- Gradio for amazing UI framework

## ğŸ“ Support

- **Documentation**: See `/deployment` folder
- **Issues**: [GitHub Issues](https://github.com/yourusername/multimodal-genai-studio/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/multimodal-genai-studio/discussions)

## ğŸš€ Roadmap

- [ ] Video generation support
- [ ] Real-time streaming UI
- [ ] Multi-user support
- [ ] API endpoints
- [ ] More pipeline templates
- [ ] Fine-tuning support

---

**Built with â¤ï¸ to showcase multimodal AI expertise**

**Demo**: [Live Demo URL]  
**Portfolio**: [Your Portfolio]  
**LinkedIn**: [Your LinkedIn]

